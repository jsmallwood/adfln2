{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"factoryName": {
			"type": "string",
			"metadata": "Data Factory name",
			"defaultValue": "adfln2"
		},
		"ADFPickup_properties_typeProperties_fileName": {
			"type": "string",
			"defaultValue": ""
		},
		"ADFPickup_properties_typeProperties_folderPath": {
			"type": "string",
			"defaultValue": ""
		},
		"FileServerDataSourceStore1_properties_typeProperties_fileName": {
			"type": "string",
			"defaultValue": ""
		},
		"FileServerDataSourceStore1_properties_typeProperties_folderPath": {
			"type": "string",
			"defaultValue": "@dataset().FolderPath"
		}
	},
	"variables": {
		"factoryId": "[concat('Microsoft.DataFactory/factories/', parameters('factoryName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('factoryName'), '/ETL with Azure Databricks')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "Run a simple ETL job using Azure Databricks, with a single pane of glass monitoring from ADF.\n\nIn the template, we check for source dataset availability. Once it is available we copy it into a blob storage for staging using a Copy activity. The same storage is accessed from Databricks clusters while processing the data (ETL). The output is stored in the same storage under 'output' folder. Various notebook properties are referenced as expressions using pipeline parameters, which lets you configure more generic and reusable pipelines. \n \nFor steps on setting up storage and databricks notebook refer https://aka.ms/databricks-instructions. ",
				"activities": [
					{
						"name": "Availability flag",
						"description": "Lookup (or Get Metadata) activity is used to get information about the source files if they are available for processing. \nIn this template, only when the '_success' flag/ file is available at source, would the downstream activities be triggered. ",
						"type": "Lookup",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "BlobSource",
								"recursive": true
							},
							"dataset": {
								"referenceName": "SourceAvailabilityBlobStore1",
								"type": "DatasetReference",
								"parameters": {}
							}
						}
					},
					{
						"name": "file-to-blob",
						"description": "Copy activity copies the actual files/ dataset to be processed by Databricks into a staging store. This storage should be accessible by the Azure Databricks cluster referenced in the next activity. ",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "Availability flag",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "BlobSource",
								"recursive": true
							},
							"sink": {
								"type": "BlobSink"
							},
							"enableStaging": false,
							"dataIntegrationUnits": 0
						},
						"inputs": [
							{
								"referenceName": "SourceFilesBlobStore1",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "DestinationFilesBlobStore1",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "ETL",
						"description": "Databricks Notebook activity does the processing of the data copied in the previous step (copy activity).  Please ensure you have added the databricks notebook (<a href='https://adflabstaging1.blob.core.windows.net/share/Transformations.html' target='_blank'>https://adflabstaging1.blob.core.windows.net/share/Transformations.html</a>) in the databricks work-space and referenced it in the notebook activity in ADF.",
						"type": "DatabricksNotebook",
						"dependsOn": [
							{
								"activity": "file-to-blob",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebookPath": "/Transformations",
							"baseParameters": {
								"input": {
									"value": "@pipeline().parameters.inputPath",
									"type": "Expression"
								},
								"output": {
									"value": "@pipeline().parameters.outputPath",
									"type": "Expression"
								},
								"filename": {
									"value": "@pipeline().parameters.fileName",
									"type": "Expression"
								},
								"pipelineRunId": {
									"value": "@pipeline().RunId",
									"type": "Expression"
								}
							}
						},
						"linkedServiceName": {
							"referenceName": "lgndatabricks",
							"type": "LinkedServiceReference"
						}
					}
				],
				"parameters": {
					"inputPath": {
						"type": "String",
						"defaultValue": "/staged_sink"
					},
					"outputPath": {
						"type": "String",
						"defaultValue": "/processed_sink"
					},
					"fileName": {
						"type": "String",
						"defaultValue": "Product.csv"
					}
				},
				"annotations": []
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/IncrementalCopyPipeline')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "LookupOldWaterMarkActivity",
						"type": "Lookup",
						"dependsOn": [
							{
								"activity": "InsertNewData",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "SqlSource"
							},
							"dataset": {
								"referenceName": "WatermarkDataset",
								"type": "DatasetReference",
								"parameters": {}
							},
							"firstRowOnly": true
						}
					},
					{
						"name": "LookupNewWaterMarkActivity",
						"type": "Lookup",
						"dependsOn": [
							{
								"activity": "InsertNewData",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "SqlSource",
								"sqlReaderQuery": "select MAX(LastModifytime) as NewWatermarkvalue from data_source_table"
							},
							"dataset": {
								"referenceName": "SourceDataset",
								"type": "DatasetReference",
								"parameters": {}
							}
						}
					},
					{
						"name": "IncrementalCopyActivity",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "LookupOldWaterMarkActivity",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "LookupNewWaterMarkActivity",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "SqlSource",
								"sqlReaderQuery": {
									"value": "select * from data_source_table where LastModifytime > '@{activity('LookupOldWaterMarkActivity').output.firstRow.WatermarkValue}' and LastModifytime <= '@{activity('LookupNewWaterMarkActivity').output.firstRow.NewWatermarkvalue}'",
									"type": "Expression"
								}
							},
							"sink": {
								"type": "DelimitedTextSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "DelimitedTextWriteSettings",
									"quoteAllText": true,
									"fileExtension": ".csv"
								}
							},
							"enableStaging": false,
							"dataIntegrationUnits": 0
						},
						"inputs": [
							{
								"referenceName": "SourceDataset",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "SpatialSink",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "StoredProceduretoWriteWatermarkActivity",
						"type": "SqlServerStoredProcedure",
						"dependsOn": [
							{
								"activity": "IncrementalCopyActivity",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"storedProcedureName": "[[dbo].[sp_write_watermark]",
							"storedProcedureParameters": {
								"LastModifiedtime": {
									"value": {
										"value": "@{activity('LookupNewWaterMarkActivity').output.firstRow.NewWatermarkvalue}",
										"type": "Expression"
									},
									"type": "DateTime"
								},
								"TableName": {
									"value": {
										"value": "@{activity('LookupOldWaterMarkActivity').output.firstRow.TableName}",
										"type": "Expression"
									},
									"type": "String"
								}
							}
						},
						"linkedServiceName": {
							"referenceName": "AzureSqlDatabaseLinkedService",
							"type": "LinkedServiceReference"
						}
					},
					{
						"name": "InsertNewData",
						"type": "SqlServerStoredProcedure",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"storedProcedureName": "[[dbo].[usp_insertdatasourcetable]"
						},
						"linkedServiceName": {
							"referenceName": "AzureSqlDatabaseLinkedService",
							"type": "LinkedServiceReference"
						}
					}
				],
				"folder": {
					"name": "Copy Demos"
				},
				"annotations": []
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/IterateAndCopyFredExports')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "IterateRequestBodyUrls",
						"type": "ForEach",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@pipeline().parameters.RequestBodyUrlList",
								"type": "Expression"
							},
							"isSequential": true,
							"activities": [
								{
									"name": "CopyFredData",
									"type": "Copy",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "DelimitedTextSource",
											"storeSettings": {
												"type": "HttpReadSettings",
												"requestMethod": "GET",
												"requestBody": "id=CSUSHPINSA&cosd=2018-06-28&coed=2018-10-01",
												"requestTimeout": ""
											},
											"formatSettings": {
												"type": "DelimitedTextReadSettings"
											}
										},
										"sink": {
											"type": "DelimitedTextSink",
											"storeSettings": {
												"type": "AzureBlobFSWriteSettings"
											},
											"formatSettings": {
												"type": "DelimitedTextWriteSettings",
												"quoteAllText": true,
												"fileExtension": ".txt"
											}
										},
										"enableStaging": false
									},
									"inputs": [
										{
											"referenceName": "FREDSource",
											"type": "DatasetReference",
											"parameters": {
												"paramone": {
													"value": "@pipeline().parameters.BaseUrl",
													"type": "Expression"
												}
											}
										}
									],
									"outputs": [
										{
											"referenceName": "FREDSink",
											"type": "DatasetReference",
											"parameters": {}
										}
									]
								}
							]
						}
					}
				],
				"parameters": {
					"RequestBodyUrlList": {
						"type": "array"
					},
					"BaseUrl": {
						"type": "array"
					}
				},
				"folder": {
					"name": "Copy Demos/Federal Reserve"
				},
				"annotations": []
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/IterateAndCopySQLTables')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "IterateSQLTables",
						"type": "ForEach",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@pipeline().parameters.tableList",
								"type": "Expression"
							},
							"isSequential": true,
							"activities": [
								{
									"name": "Copy Data",
									"type": "Copy",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "SqlSource",
											"sqlReaderQuery": {
												"value": "SELECT * FROM [@{item().TABLE_SCHEMA}].[@{item().TABLE_NAME}]",
												"type": "Expression"
											}
										},
										"sink": {
											"type": "DelimitedTextSink",
											"storeSettings": {
												"type": "AzureBlobStorageWriteSettings"
											},
											"formatSettings": {
												"type": "DelimitedTextWriteSettings",
												"quoteAllText": true,
												"fileExtension": ".csv"
											}
										},
										"enableStaging": true,
										"stagingSettings": {
											"linkedServiceName": {
												"referenceName": "MyAzureBlobLinkedService",
												"type": "LinkedServiceReference"
											}
										},
										"dataIntegrationUnits": 0
									},
									"inputs": [
										{
											"referenceName": "AzureSqlDatabaseDataset",
											"type": "DatasetReference",
											"parameters": {}
										}
									],
									"outputs": [
										{
											"referenceName": "DelimitedText1",
											"type": "DatasetReference",
											"parameters": {}
										}
									]
								}
							]
						}
					}
				],
				"parameters": {
					"tableList": {
						"type": "Array"
					}
				},
				"folder": {
					"name": "Copy Demos"
				},
				"annotations": []
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/ADFPickup')]",
			"type": "Microsoft.DataFactory/factories/datasets",
			"apiVersion": "2018-06-01",
			"properties": {
				"linkedServiceName": {
					"referenceName": "LGNP50",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "FileShare",
				"typeProperties": {
					"fileName": "[parameters('ADFPickup_properties_typeProperties_fileName')]",
					"folderPath": "[parameters('ADFPickup_properties_typeProperties_folderPath')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/FileServerDataSourceStore1')]",
			"type": "Microsoft.DataFactory/factories/datasets",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "Connection to your data source store.  ",
				"linkedServiceName": {
					"referenceName": "LGNP50",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"FolderPath": {
						"type": "String"
					},
					"LastModified_From": {
						"type": "String"
					},
					"LastModified_To": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "FileShare",
				"typeProperties": {
					"fileName": "[parameters('FileServerDataSourceStore1_properties_typeProperties_fileName')]",
					"folderPath": {
						"value": "[parameters('FileServerDataSourceStore1_properties_typeProperties_folderPath')]",
						"type": "Expression"
					},
					"modifiedDatetimeStart": {
						"value": "@dataset().LastModified_From",
						"type": "Expression"
					},
					"modifiedDatetimeEnd": {
						"value": "@dataset().LastModified_To",
						"type": "Expression"
					},
					"key": "*",
					"bucketName": {
						"value": "@dataset().FolderPath",
						"type": "Expression"
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/Currency Converter1')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "Sample demo data flow to convert currencies",
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "CurrencyDatasetUSD1",
								"type": "DatasetReference"
							},
							"name": "USDCurrency",
							"typeProperties": {}
						},
						{
							"dataset": {
								"referenceName": "CurrencyDatasetCAD1",
								"type": "DatasetReference"
							},
							"name": "CADSource",
							"typeProperties": {}
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "USDOutput1",
								"type": "DatasetReference"
							},
							"name": "USDSink"
						},
						{
							"dataset": {
								"referenceName": "CADOutput1",
								"type": "DatasetReference"
							},
							"name": "CADSink"
						}
					],
					"transformations": [
						{
							"name": "Union",
							"description": "The Union combines 2 streams together"
						},
						{
							"name": "NewCurrencyColumn",
							"description": "Create a new calculated column from currency rate"
						},
						{
							"name": "ConditionalSplit1",
							"description": "Split the data on state to create 2 streams"
						}
					],
					"script": "\n\nsource(output(\n\t\tColumn_1 as string,\n\t\tColumn_2 as string,\n\t\tColumn_3 as string,\n\t\tColumn_4 as string\n\t),\n\tallowSchemaDrift: false,\n\tvalidateSchema: false) ~> USDCurrency\nsource(output(\n\t\tPreviousConversionRate as double,\n\t\tCountry as string,\n\t\tDateTime1 as string,\n\t\tCurrentConversionRate as double\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false) ~> CADSource\nUSDCurrency, CADSource union(byName: true)~> Union\nUnion derive(NewCurrencyRate = round(CurrentConversionRate*1.25)) ~> NewCurrencyColumn\nNewCurrencyColumn split(Country == 'USD',\n\tCountry == 'CAD',\n\tdisjoint: false) ~> ConditionalSplit1@(USD, CAD)\nConditionalSplit1@USD sink(allowSchemaDrift: true,\n\tvalidateSchema: false) ~> USDSink\nConditionalSplit1@CAD sink(allowSchemaDrift: true,\n\tvalidateSchema: false) ~> CADSink"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/Fixed Width1')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "Each parameter in this data flow is defined as 'start position', 'offset' as in '1,7'.",
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "fixedwidth1",
								"type": "DatasetReference"
							},
							"name": "fixedsource1",
							"typeProperties": {}
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "folderout1",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "MapFields"
						}
					],
					"script": "\nparameters{\n\tField1 as string ('1,7'),\n\tField2 as string ('8,8'),\n\tField3 as string ('15,10'),\n\tField4 as string ('25,11'),\n\tField5 as string ('36,10'),\n\tField6 as string ('46,12'),\n\tField7 as string ('58,1')\n}\nsource(output(\n\t\tColumn_1 as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false) ~> fixedsource1\nfixedsource1 derive(Field1 = substring(Column_1,toInteger(split($Field1,',')[1]),toInteger(split($Field1,',')[2])),\n\t\tField2 = substring(Column_1,toInteger(split($Field2,',')[1]),toInteger(split($Field2,',')[2])),\n\t\tField3 = substring(Column_1,toInteger(split($Field3,',')[1]),toInteger(split($Field3,',')[2])),\n\t\tField4 = substring(Column_1,toInteger(split($Field4,',')[1]),toInteger(split($Field4,',')[2])),\n\t\tField5 = substring(Column_1,toInteger(split($Field5,',')[1]),toInteger(split($Field5,',')[2])),\n\t\tField6 = substring(Column_1,toInteger(split($Field6,',')[1]),toInteger(split($Field6,',')[2])),\n\t\tField7 = substring(Column_1,toInteger(split($Field7,',')[1]),toInteger(split($Field7,',')[2])),\n\tpartitionBy('roundRobin', 2)) ~> MapFields\nMapFields sink(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tpartitionFileNames:['mysinglefile.csv'],\n\ttruncate: true,\n\tpartitionBy('hash', 1)) ~> sink1"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/MovieDemo1')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "movie_dataflow_source1",
								"type": "DatasetReference"
							},
							"name": "Movies",
							"typeProperties": {}
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "movie_dataflow_sink1",
								"type": "DatasetReference"
							},
							"name": "Output"
						}
					],
					"transformations": [
						{
							"name": "MoviesYear"
						}
					],
					"script": "\n\nsource(output(\n\t\tmovieId as string,\n\t\ttitle as string,\n\t\tgenres as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false) ~> Movies\nMovies derive(year = toInteger(trim(right(title, 6), '()')),\n\t\ttitle = toString(left(title, length(title)-6))) ~> MoviesYear\nMoviesYear sink(allowSchemaDrift: true,\n\tvalidateSchema: false) ~> Output"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/RankMovies')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "moviesDB",
								"type": "DatasetReference"
							},
							"name": "source1",
							"typeProperties": {}
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "RankedMovieSink",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "DerivedColumn1"
						},
						{
							"name": "Window1"
						},
						{
							"name": "Select1"
						}
					],
					"script": "\n\nsource(output(\n\t\tmovie as string,\n\t\ttitle as string,\n\t\tgenres as string,\n\t\tyear as integer,\n\t\tRating as integer,\n\t\t{Rotton Tomato} as integer\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false) ~> source1\nsource1 derive(dummy = 1,\n\t\tcombo = (Rating * 5) + {Rotton Tomato}) ~> DerivedColumn1\nDerivedColumn1 window(over(dummy),\n\tasc(combo, true),\n\tRANK = rank(combo)) ~> Window1\nWindow1 select(mapColumn(\n\t\tmovie,\n\t\ttitle,\n\t\tgenres,\n\t\tyear,\n\t\tRating,\n\t\tRottenTomato = {Rotton Tomato},\n\t\tdummy,\n\t\tcombo,\n\t\tRANK\n\t),\n\tskipDuplicateMapInputs: false,\n\tskipDuplicateMapOutputs: false) ~> Select1\nSelect1 sink(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tformat: 'parquet') ~> sink1"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/STLFED')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "StagingParquet1",
								"type": "DatasetReference"
							},
							"name": "STLPCPI",
							"typeProperties": {}
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "AzureSqlDatabaseDataset_New1",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "ConverToInts"
						}
					],
					"script": "\n\nsource(output(\n\t\tDATE as string,\n\t\tSTLPCPI_20171116 as string,\n\t\tSTLPCPI_20181115 as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tformat: 'parquet') ~> STLPCPI\nSTLPCPI derive(DATE = toDate(DATE),\n\t\tSTLPCPI_20171116 = toInteger(STLPCPI_20171116),\n\t\tSTLPCPI_20181115 = toInteger(STLPCPI_20181115)) ~> ConverToInts\nConverToInts sink(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tdeletable:false,\n\tinsertable:true,\n\tupdateable:false,\n\tupsertable:false,\n\tformat: 'table') ~> sink1"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/SearchLog1')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "searchLog1",
								"type": "DatasetReference"
							},
							"name": "searchLog",
							"typeProperties": {}
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "AzureSqlDWTable11_New1",
								"type": "DatasetReference"
							},
							"name": "sinkIntoDW"
						}
					],
					"transformations": [
						{
							"name": "totalDurationByRegion"
						},
						{
							"name": "RenameColumns"
						},
						{
							"name": "DateFilter"
						},
						{
							"name": "ConvertDate"
						},
						{
							"name": "DurationFilter"
						}
					],
					"script": "\n\nsource(output(\n\t\t{_col0_} as integer,\n\t\t{_col1_} as string,\n\t\t{_col2_} as string,\n\t\t{_col3_} as string,\n\t\t{_col4_} as integer,\n\t\t{_col5_} as string,\n\t\t{_col6_} as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false) ~> searchLog\nDateFilter aggregate(groupBy(region),\n\ttotalduration = sum(duration)) ~> totalDurationByRegion\nsearchLog select(mapColumn(\n\t\tuserid = {_col0_},\n\t\tstart = {_col1_},\n\t\tregion = {_col2_},\n\t\tquery = {_col3_},\n\t\tduration = {_col4_},\n\t\turls = {_col5_},\n\t\tclickedurls = {_col6_}\n\t),\n\tskipDuplicateMapInputs: false,\n\tskipDuplicateMapOutputs: false) ~> RenameColumns\nConvertDate filter(newdate > toDate('2012-02-06','yyyy-MM-dd')) ~> DateFilter\nRenameColumns derive(newdate = toDate(left(start,instr(start,' ')-1),'MM/dd/yyyy')) ~> ConvertDate\ntotalDurationByRegion filter(totalduration > 200) ~> DurationFilter\nDurationFilter sink(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tdeletable:false,\n\tinsertable:true,\n\tupdateable:false,\n\tupsertable:false,\n\tformat: 'table',\n\tstaged: false) ~> sinkIntoDW"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/TaxiDemo1')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "taxi_trip_data_input1",
								"type": "DatasetReference"
							},
							"name": "TripData",
							"typeProperties": {}
						},
						{
							"dataset": {
								"referenceName": "taxi_trip_fare_input1",
								"type": "DatasetReference"
							},
							"name": "TripFare",
							"typeProperties": {}
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "TaxiDemoVendorStatsSink1",
								"type": "DatasetReference"
							},
							"name": "VendorStatsSink"
						},
						{
							"dataset": {
								"referenceName": "TaxiDemoDayStatsSink1",
								"type": "DatasetReference"
							},
							"name": "DayStatsSink"
						},
						{
							"dataset": {
								"referenceName": "TaxiDemoTotalByPaymentType1",
								"type": "DatasetReference"
							},
							"name": "TotalPaymentByPaymentType"
						}
					],
					"transformations": [
						{
							"name": "JoinMatchedData"
						},
						{
							"name": "AggregateVendorStats"
						},
						{
							"name": "AggregateDayStats"
						},
						{
							"name": "AggregateByPaymentType"
						}
					],
					"script": "\n\nsource(output(\n\t\tmedallion as string,\n\t\thack_license as string,\n\t\tvendor_id as string,\n\t\trate_code as string,\n\t\tstore_and_fwd_flag as string,\n\t\tpickup_datetime as string,\n\t\tdropoff_datetime as string,\n\t\tpassenger_count as short,\n\t\ttrip_time_in_secs as long,\n\t\ttrip_distance as double,\n\t\tpickup_longitude as double,\n\t\tpickup_latitude as double,\n\t\tdropoff_longitude as double,\n\t\tdropoff_latitude as double\n\t),\n\tallowSchemaDrift: false,\n\tvalidateSchema: false) ~> TripData\nsource(output(\n\t\tmedallion as string,\n\t\t{ hack_license} as string,\n\t\t{ vendor_id} as string,\n\t\t{ pickup_datetime} as string,\n\t\t{ payment_type} as string,\n\t\t{ fare_amount} as double,\n\t\t{ surcharge} as double,\n\t\t{ mta_tax} as double,\n\t\t{ tip_amount} as double,\n\t\t{ tolls_amount} as double,\n\t\t{ total_amount} as double\n\t),\n\tallowSchemaDrift: false,\n\tvalidateSchema: false) ~> TripFare\nTripData, TripFare join(hack_license == { hack_license}\n\t&& TripData@medallion == TripFare@medallion\n\t&& vendor_id == { vendor_id}\n\t&& pickup_datetime == { pickup_datetime},\n\tjoinType:'inner',\n\tbroadcast: 'none')~> JoinMatchedData\nJoinMatchedData aggregate(groupBy(vendor_id),\n\tpassenger_count = round(sum(passenger_count), 2),\n\t\ttrip_time_in_secs = round(sum(trip_time_in_secs)/60, 2),\n\t\ttrip_distance = round(sum(trip_distance), 2),\n\t\tTotalTripFare = round(sum({ total_amount}), 2)) ~> AggregateVendorStats\nJoinMatchedData aggregate(groupBy(DayOfTheWeek = dayOfWeek(toDate(pickup_datetime,'yyyy-mm-dd hh:mm:ss'))),\n\ttrip_distance = round(avg(trip_distance), 2),\n\t\tpassenger_count = round(avg(passenger_count), 2),\n\t\ttrip_time_in_secs = round(avg(trip_time_in_secs)/60, 2),\n\t\taverage_fare = round(avg({ total_amount}), 2)) ~> AggregateDayStats\nTripFare aggregate(groupBy({ payment_type}),\n\teach(match(type=='double'), concat($$, '_total') = round(sum ($$)))) ~> AggregateByPaymentType\nAggregateVendorStats sink(allowSchemaDrift: true,\n\tvalidateSchema: false) ~> VendorStatsSink\nAggregateDayStats sink(allowSchemaDrift: true,\n\tvalidateSchema: false) ~> DayStatsSink\nAggregateByPaymentType sink(allowSchemaDrift: true,\n\tvalidateSchema: false) ~> TotalPaymentByPaymentType"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/WranglingDataFlow1')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "WranglingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"name": "nyctaxi_source",
							"script": "",
							"dataset": {
								"referenceName": "nyctaxi_source",
								"type": "DatasetReference"
							},
							"typeProperties": {
								"readMode": null
							}
						}
					],
					"sinks": [
						{
							"name": "nyctaxisink",
							"dataset": {
								"referenceName": "nyctaxisink",
								"type": "DatasetReference"
							},
							"script": "sink(allowSchemaDrift: true,\n\tvalidateSchema: false) ~> nyctaxisink"
						}
					],
					"script": "section Section1;\r\nshared nyctaxi_source = let\r\n  AdfDoc = Web.Contents(\"https://lgndatalake.dfs.core.windows.net/raw/US/nyctaxi/2016/yellow_tripdata_2016-06.txt?sv=2018-03-28&sig=FCsfMni4FIk2adPW9bMcS7P5cC4pOpIRnGHfCV9sBvg%3D&spr=https&se=2019-09-25T21%3A20%3A59Z&srt=sco&ss=bf&sp=rwl\"),\r\n  Csv = Csv.Document(AdfDoc, [Delimiter = \",\", Encoding = TextEncoding.Utf8, QuoteStyle = QuoteStyle.None]),\r\n  PromotedHeaders = Table.PromoteHeaders(Csv, [PromoteAllScalars = true]),\r\n  TrimmedHeaders = Table.TransformColumnNames(PromotedHeaders, (columnName as text) as text => Text.Trim(columnName))\r\nin\r\n  TrimmedHeaders;\r\nshared UserQuery = let\r\n  Source = nyctaxi_source\r\nin\r\n  Source;\r\n"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/dedupeProb21')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "names1001",
								"type": "DatasetReference"
							},
							"name": "sourceName",
							"typeProperties": {}
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "dupefolder1",
								"type": "DatasetReference"
							},
							"name": "sinkDupes"
						},
						{
							"dataset": {
								"referenceName": "dupefolder1",
								"type": "DatasetReference"
							},
							"name": "sinkNoDupes"
						}
					],
					"transformations": [
						{
							"name": "FuzzyMatch"
						},
						{
							"name": "groupSoundex"
						},
						{
							"name": "Orig1"
						},
						{
							"name": "soundexJoin"
						},
						{
							"name": "soundexBranch"
						},
						{
							"name": "groupPhone"
						},
						{
							"name": "phoneBranch"
						},
						{
							"name": "phoneJoin"
						},
						{
							"name": "groupZip"
						},
						{
							"name": "zipBranch"
						},
						{
							"name": "zipJoin"
						},
						{
							"name": "setConstants"
						},
						{
							"name": "matchScore"
						},
						{
							"name": "finalResult"
						},
						{
							"name": "MapNames"
						},
						{
							"name": "CreateFullName"
						},
						{
							"name": "CheckForDupes"
						}
					],
					"script": "\n\nsource(output(\n\t\t{Emp ID} as integer,\n\t\t{Name Prefix} as string,\n\t\t{First Name} as string,\n\t\t{Middle Initial} as string,\n\t\t{Last Name} as string,\n\t\tGender as string,\n\t\t{E Mail} as string,\n\t\t{Father's Name} as string,\n\t\t{Mother's Name} as string,\n\t\t{Mother's Maiden Name} as string,\n\t\t{Date of Birth} as date,\n\t\t{Time of Birth} as string,\n\t\t{Age in Yrs.} as double,\n\t\t{Weight in Kgs.} as integer,\n\t\t{Date of Joining} as date,\n\t\t{Quarter of Joining} as string,\n\t\t{Half of Joining} as string,\n\t\t{Year of Joining} as short,\n\t\t{Month of Joining} as short,\n\t\t{Month Name of Joining} as string,\n\t\t{Short Month} as string,\n\t\t{Day of Joining} as short,\n\t\t{DOW of Joining} as string,\n\t\t{Short DOW} as string,\n\t\t{Age in Company (Years)} as double,\n\t\tSalary as integer,\n\t\t{Last % Hike} as string,\n\t\tSSN as string,\n\t\t{Phone No. } as string,\n\t\t{Place Name} as string,\n\t\tCounty as string,\n\t\tCity as string,\n\t\tState as string,\n\t\tZip as integer,\n\t\tRegion as string,\n\t\t{User Name} as string,\n\t\tPassword as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false) ~> sourceName\nMapNames derive(SoundexValue = soundex(fullname)) ~> FuzzyMatch\nOrig1 aggregate(groupBy(SoundexValue),\n\tsoundexmatch = sum(1)) ~> groupSoundex\nFuzzyMatch select(mapColumn(\n\t\tacctnum,\n\t\tfullname,\n\t\tphone,\n\t\tzip,\n\t\tSoundexValue\n\t),\n\tskipDuplicateMapInputs: false,\n\tskipDuplicateMapOutputs: false) ~> Orig1\ngroupSoundex, soundexBranch join(groupSoundex@SoundexValue == soundexBranch@SoundexValue,\n\tjoinType:'inner',\n\tbroadcast: 'none')~> soundexJoin\nFuzzyMatch select(mapColumn(\n\t\tacctnum,\n\t\tfullname,\n\t\tphone,\n\t\tzip,\n\t\tSoundexValue\n\t),\n\tskipDuplicateMapInputs: false,\n\tskipDuplicateMapOutputs: false) ~> soundexBranch\nsoundexJoin aggregate(groupBy(phone,\n\t\tsoundexBranch@SoundexValue),\n\tphonematch = sum(1),\n\t\tacctnum_agg = last(acctnum)) ~> groupPhone\nsoundexJoin select(mapColumn(\n\t\tsoundexmatch,\n\t\tacctnum,\n\t\tfullname,\n\t\tphone,\n\t\tzip,\n\t\tSoundexValue = soundexBranch@SoundexValue\n\t),\n\tskipDuplicateMapInputs: false,\n\tskipDuplicateMapOutputs: false) ~> phoneBranch\ngroupPhone, phoneBranch join(acctnum_agg == acctnum,\n\tjoinType:'right',\n\tbroadcast: 'none')~> phoneJoin\nphoneJoin aggregate(groupBy(zip,\n\t\tphoneBranch@SoundexValue),\n\tzipcount = sum(1),\n\t\tacctnum_agg = last(acctnum_agg)) ~> groupZip\nphoneJoin select(mapColumn(\n\t\tphonematch,\n\t\tsoundexmatch,\n\t\tacctnum,\n\t\tfullname,\n\t\tphone = phoneBranch@phone,\n\t\tzip,\n\t\tSoundexValue = phoneBranch@SoundexValue\n\t),\n\tskipDuplicateMapInputs: false,\n\tskipDuplicateMapOutputs: false) ~> zipBranch\ngroupZip, zipBranch join(acctnum_agg == acctnum,\n\tjoinType:'right',\n\tbroadcast: 'none')~> zipJoin\nzipJoin derive(soundexweight = 50,\n\t\tzipweight = 25,\n\t\tphoneweight = 25,\n\t\tsoundexbool = iif (soundexmatch > 1, 1, 0),\n\t\tzipbool = iif (zipcount > 1, 1, 0),\n\t\tphonebool = iif (phonematch > 1, 1, 0)) ~> setConstants\nsetConstants derive(matchscore = (soundexbool * 50) + (zipbool * 25) + (phonebool * 25)) ~> matchScore\nmatchScore select(mapColumn(\n\t\tphone,\n\t\tacctnum,\n\t\tfullname,\n\t\tzip = zipBranch@zip,\n\t\tmatchscore\n\t),\n\tskipDuplicateMapInputs: false,\n\tskipDuplicateMapOutputs: false) ~> finalResult\nCreateFullName select(mapColumn(\n\t\tphone = {Phone No. },\n\t\tzip = Zip,\n\t\tfullname,\n\t\tacctnum = {Emp ID}\n\t),\n\tskipDuplicateMapInputs: false,\n\tskipDuplicateMapOutputs: false) ~> MapNames\nsourceName derive(fullname = {First Name} + ' ' + {Last Name}) ~> CreateFullName\nfinalResult split(matchscore > 50,\n\tdisjoint: false) ~> CheckForDupes@(Duplicates, NotDupe)\nCheckForDupes@Duplicates sink(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tpartitionFileNames:['dupes.csv'],\n\tpartitionBy('hash', 1)) ~> sinkDupes\nCheckForDupes@NotDupe sink(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tpartitionFileNames:['nodupes.csv'],\n\tpartitionBy('hash', 1)) ~> sinkNoDupes"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/CopyBinaryOnPremToBlob')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "CopyJPGtoBlob",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "FileSystemSource",
								"recursive": true
							},
							"sink": {
								"type": "BlobSink"
							},
							"enableStaging": false
						},
						"inputs": [
							{
								"referenceName": "ADFPickup",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "adfdropoff",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					}
				],
				"folder": {
					"name": "Copy Demos"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/datasets/ADFPickup')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/CopyNewFilesByLastModifiedDate1')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "Copy new and changed files only by using LastModifiedDate",
				"activities": [
					{
						"name": "CopyNewFiles",
						"description": "Copy new and changed files only by using LastModifiedDate",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "FileSystemSource",
								"recursive": true
							},
							"sink": {
								"type": "AzureBlobFSSink"
							},
							"enableStaging": false
						},
						"inputs": [
							{
								"referenceName": "FileServerDataSourceStore1",
								"type": "DatasetReference",
								"parameters": {
									"FolderPath": {
										"value": "@pipeline().parameters.FolderPath_Source",
										"type": "Expression"
									},
									"LastModified_From": {
										"value": "@pipeline().parameters.LastModified_From",
										"type": "Expression"
									},
									"LastModified_To": {
										"value": "@pipeline().parameters.LastModified_To",
										"type": "Expression"
									}
								}
							}
						],
						"outputs": [
							{
								"referenceName": "AzureBlobFSDataDestinationStore1",
								"type": "DatasetReference",
								"parameters": {
									"FolderPath": {
										"value": "@pipeline().parameters.FolderPath_Destination",
										"type": "Expression"
									}
								}
							}
						]
					}
				],
				"parameters": {
					"FolderPath_Source": {
						"type": "String",
						"defaultValue": "/<myfolder>/<sub_folder>/"
					},
					"FolderPath_Destination": {
						"type": "String",
						"defaultValue": "/<myfolder>/<sub_folder>/"
					},
					"LastModified_From": {
						"type": "String",
						"defaultValue": "2019-02-01T00:00:00Z"
					},
					"LastModified_To": {
						"type": "String",
						"defaultValue": "2019-03-01T00:00:00Z"
					}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/datasets/FileServerDataSourceStore1')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/CurrencyConverter')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "Currency Converter1",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "Currency Converter1",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"USDCurrency": {},
									"CADSource": {},
									"USDSink": {},
									"CADSink": {}
								}
							},
							"staging": {}
						}
					}
				],
				"folder": {
					"name": "Data Flow Demos"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/dataflows/Currency Converter1')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/Dedupe Pipeline')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "Deduplication Data Flow",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "dedupeProb21",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"sourceName": {},
									"sinkDupes": {},
									"sinkNoDupes": {}
								}
							},
							"staging": {}
						}
					}
				],
				"folder": {
					"name": "Data Flow Demos"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/dataflows/dedupeProb21')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/Fixed Width Data Flow Pipeline1')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "This is an example of creating a data flow to process fixed-width text files",
				"activities": [
					{
						"name": "fixedwidth",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "Fixed Width1",
								"type": "DataFlowReference",
								"parameters": {
									"Field1": "'1,7'",
									"Field2": "'8,8'",
									"Field3": "'15,10'",
									"Field4": "'25,11'",
									"Field5": "'36,10'",
									"Field6": "'46,12'",
									"Field7": "'58,1'"
								},
								"datasetParameters": {
									"fixedsource1": {},
									"sink1": {}
								}
							},
							"staging": {}
						}
					}
				],
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/dataflows/Fixed Width1')]"
			]
		}
	]
}