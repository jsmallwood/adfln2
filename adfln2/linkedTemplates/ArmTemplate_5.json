{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"factoryName": {
			"type": "string",
			"metadata": "Data Factory name",
			"defaultValue": "adfln2"
		},
		"lnmyserver_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'lnmyserver'"
		}
	},
	"variables": {
		"factoryId": "[concat('Microsoft.DataFactory/factories/', parameters('factoryName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('factoryName'), '/CopyPipeline_z0a')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "Copy_z0a",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [
							{
								"name": "Source",
								"value": "raw//hciMastIncdntv6_hciMasterIncideObject_1900-01-01_2018-12-31.json"
							},
							{
								"name": "Destination",
								"value": "dbo.jsontest"
							}
						],
						"typeProperties": {
							"source": {
								"type": "JsonSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": false
								},
								"formatSettings": {
									"type": "JsonReadSettings"
								}
							},
							"sink": {
								"type": "AzureSqlSink",
								"tableOption": "autoCreate"
							},
							"enableStaging": false,
							"validateDataConsistency": false
						},
						"inputs": [
							{
								"referenceName": "SourceDataset_z0a",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "DestinationDataset_z0a",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					}
				],
				"folder": {
					"name": "Archive"
				},
				"annotations": []
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/CopyTablesToParquetFull')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "Get-Tables",
						"type": "Lookup",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "AzureSqlSource",
								"sqlReaderQuery": "SELECT * FROM dbo.DataFactoryMeta WHERE active = 1",
								"queryTimeout": "02:00:00",
								"partitionOption": "None"
							},
							"dataset": {
								"referenceName": "AzureSqlGeneric",
								"type": "DatasetReference",
								"parameters": {}
							},
							"firstRowOnly": false
						}
					},
					{
						"name": "Copy-Each-Table",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Get-Tables",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('Get-Tables').output.value",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "Copy-Table",
									"type": "Copy",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "AzureSqlSource",
											"sqlReaderQuery": {
												"value": "SELECT * FROM @{item().TableName}",
												"type": "Expression"
											},
											"queryTimeout": "02:00:00",
											"partitionOption": "None"
										},
										"sink": {
											"type": "ParquetSink",
											"storeSettings": {
												"type": "AzureBlobFSWriteSettings"
											},
											"formatSettings": {
												"type": "ParquetWriteSettings"
											}
										},
										"enableStaging": false,
										"translator": {
											"type": "TabularTranslator",
											"typeConversion": true,
											"typeConversionSettings": {
												"allowDataTruncation": true,
												"treatBooleanAsNumber": false
											}
										}
									},
									"inputs": [
										{
											"referenceName": "AzureSqlGeneric",
											"type": "DatasetReference",
											"parameters": {}
										}
									],
									"outputs": [
										{
											"referenceName": "AzureDataLakeParquet",
											"type": "DatasetReference",
											"parameters": {
												"filename": {
													"value": "@{item().TableName}",
													"type": "Expression"
												}
											}
										}
									]
								}
							]
						}
					}
				],
				"folder": {
					"name": "EHI"
				},
				"annotations": []
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/CopyTaxiData')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "lgnstorage to lgndatalake",
				"activities": [
					{
						"name": "Copy_zyh",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [
							{
								"name": "Source",
								"value": "nyctaxi//"
							},
							{
								"name": "Destination",
								"value": "raw/US/nyctaxi/"
							}
						],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "AzureBlobStorageReadSettings",
									"recursive": true,
									"wildcardFileName": "*.*"
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings",
									"skipLineCount": 0
								}
							},
							"sink": {
								"type": "DelimitedTextSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "DelimitedTextWriteSettings",
									"quoteAllText": true,
									"fileExtension": ".txt"
								}
							},
							"enableStaging": false,
							"enableSkipIncompatibleRow": true
						},
						"inputs": [
							{
								"referenceName": "SourceDataset_zyh",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "DestinationDataset_zyh",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					}
				],
				"annotations": []
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/ETL with Azure Databricks')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "Run a simple ETL job using Azure Databricks, with a single pane of glass monitoring from ADF.\n\nIn the template, we check for source dataset availability. Once it is available we copy it into a blob storage for staging using a Copy activity. The same storage is accessed from Databricks clusters while processing the data (ETL). The output is stored in the same storage under 'output' folder. Various notebook properties are referenced as expressions using pipeline parameters, which lets you configure more generic and reusable pipelines. \n \nFor steps on setting up storage and databricks notebook refer https://aka.ms/databricks-instructions. ",
				"activities": [
					{
						"name": "Availability flag",
						"description": "Lookup (or Get Metadata) activity is used to get information about the source files if they are available for processing. \nIn this template, only when the '_success' flag/ file is available at source, would the downstream activities be triggered. ",
						"type": "Lookup",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "BlobSource",
								"recursive": true
							},
							"dataset": {
								"referenceName": "SourceAvailabilityBlobStore1",
								"type": "DatasetReference",
								"parameters": {}
							}
						}
					},
					{
						"name": "file-to-blob",
						"description": "Copy activity copies the actual files/ dataset to be processed by Databricks into a staging store. This storage should be accessible by the Azure Databricks cluster referenced in the next activity. ",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "Availability flag",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "BlobSource",
								"recursive": true
							},
							"sink": {
								"type": "BlobSink"
							},
							"enableStaging": false,
							"dataIntegrationUnits": 0
						},
						"inputs": [
							{
								"referenceName": "SourceFilesBlobStore1",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "DestinationFilesBlobStore1",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "ETL",
						"description": "Databricks Notebook activity does the processing of the data copied in the previous step (copy activity).  Please ensure you have added the databricks notebook (<a href='https://adflabstaging1.blob.core.windows.net/share/Transformations.html' target='_blank'>https://adflabstaging1.blob.core.windows.net/share/Transformations.html</a>) in the databricks work-space and referenced it in the notebook activity in ADF.",
						"type": "DatabricksNotebook",
						"dependsOn": [
							{
								"activity": "file-to-blob",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebookPath": "/Transformations",
							"baseParameters": {
								"input": {
									"value": "@pipeline().parameters.inputPath",
									"type": "Expression"
								},
								"output": {
									"value": "@pipeline().parameters.outputPath",
									"type": "Expression"
								},
								"filename": {
									"value": "@pipeline().parameters.fileName",
									"type": "Expression"
								},
								"pipelineRunId": {
									"value": "@pipeline().RunId",
									"type": "Expression"
								}
							}
						},
						"linkedServiceName": {
							"referenceName": "lgndatabricks",
							"type": "LinkedServiceReference"
						}
					}
				],
				"parameters": {
					"inputPath": {
						"type": "String",
						"defaultValue": "/staged_sink"
					},
					"outputPath": {
						"type": "String",
						"defaultValue": "/processed_sink"
					},
					"fileName": {
						"type": "String",
						"defaultValue": "Product.csv"
					}
				},
				"annotations": []
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/IncrementalCopyPipeline')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "LookupOldWaterMarkActivity",
						"type": "Lookup",
						"dependsOn": [
							{
								"activity": "InsertNewData",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "SqlSource",
								"partitionOption": "None"
							},
							"dataset": {
								"referenceName": "WatermarkDataset",
								"type": "DatasetReference",
								"parameters": {}
							},
							"firstRowOnly": true
						}
					},
					{
						"name": "LookupNewWaterMarkActivity",
						"type": "Lookup",
						"dependsOn": [
							{
								"activity": "InsertNewData",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "SqlSource",
								"sqlReaderQuery": "select MAX(LastModifytime) as NewWatermarkvalue from data_source_table",
								"partitionOption": "None"
							},
							"dataset": {
								"referenceName": "SourceDataset",
								"type": "DatasetReference",
								"parameters": {}
							}
						}
					},
					{
						"name": "IncrementalCopyActivity",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "LookupOldWaterMarkActivity",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "LookupNewWaterMarkActivity",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "SqlSource",
								"sqlReaderQuery": {
									"value": "select * from data_source_table where LastModifytime > '@{activity('LookupOldWaterMarkActivity').output.firstRow.WatermarkValue}' and LastModifytime <= '@{activity('LookupNewWaterMarkActivity').output.firstRow.NewWatermarkvalue}'",
									"type": "Expression"
								},
								"partitionOption": "None"
							},
							"sink": {
								"type": "DelimitedTextSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "DelimitedTextWriteSettings",
									"quoteAllText": true,
									"fileExtension": ".csv"
								}
							},
							"enableStaging": false,
							"enableSkipIncompatibleRow": true,
							"logStorageSettings": {
								"linkedServiceName": {
									"referenceName": "lgndatalake",
									"type": "LinkedServiceReference"
								},
								"path": "dropzone"
							},
							"dataIntegrationUnits": 0
						},
						"inputs": [
							{
								"referenceName": "SourceDataset",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "SpatialSink",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "InsertNewData",
						"type": "SqlServerStoredProcedure",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 10,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"storedProcedureName": "[[dbo].[usp_insertdatasourcetable]"
						},
						"linkedServiceName": {
							"referenceName": "AzureSqlDatabaseLinkedService",
							"type": "LinkedServiceReference"
						}
					},
					{
						"name": "CheckRowcount",
						"type": "IfCondition",
						"dependsOn": [
							{
								"activity": "IncrementalCopyActivity",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"expression": {
								"value": "@activity('IncrementalCopyActivity').output.RowsCopied > 0",
								"type": "Expression"
							},
							"ifTrueActivities": [
								{
									"name": "StoredProceduretoWriteWatermarkActivity_copy1",
									"type": "SqlServerStoredProcedure",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"storedProcedureName": "[[dbo].[sp_write_watermark]",
										"storedProcedureParameters": {
											"LastModifiedtime": {
												"value": {
													"value": "@{activity('LookupNewWaterMarkActivity').output.firstRow.NewWatermarkvalue}",
													"type": "Expression"
												},
												"type": "DateTime"
											},
											"TableName": {
												"value": {
													"value": "@{activity('LookupOldWaterMarkActivity').output.firstRow.TableName}",
													"type": "Expression"
												},
												"type": "String"
											}
										}
									},
									"linkedServiceName": {
										"referenceName": "AzureSqlDatabaseLinkedService",
										"type": "LinkedServiceReference"
									}
								}
							]
						}
					}
				],
				"folder": {
					"name": "Copy Demos"
				},
				"annotations": []
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/IterateAndCopyFredExports')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "IterateRequestBodyUrls",
						"type": "ForEach",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@pipeline().parameters.RequestBodyUrlList",
								"type": "Expression"
							},
							"isSequential": true,
							"activities": [
								{
									"name": "CopyFredData",
									"type": "Copy",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "DelimitedTextSource",
											"storeSettings": {
												"type": "HttpReadSettings",
												"requestMethod": "GET",
												"requestBody": "id=CSUSHPINSA&cosd=2018-06-28&coed=2018-10-01",
												"requestTimeout": ""
											},
											"formatSettings": {
												"type": "DelimitedTextReadSettings"
											}
										},
										"sink": {
											"type": "DelimitedTextSink",
											"storeSettings": {
												"type": "AzureBlobFSWriteSettings"
											},
											"formatSettings": {
												"type": "DelimitedTextWriteSettings",
												"quoteAllText": true,
												"fileExtension": ".txt"
											}
										},
										"enableStaging": false
									},
									"inputs": [
										{
											"referenceName": "FREDSource",
											"type": "DatasetReference",
											"parameters": {
												"paramone": {
													"value": "@pipeline().parameters.BaseUrl",
													"type": "Expression"
												}
											}
										}
									],
									"outputs": [
										{
											"referenceName": "FREDSink",
											"type": "DatasetReference",
											"parameters": {}
										}
									]
								}
							]
						}
					}
				],
				"parameters": {
					"RequestBodyUrlList": {
						"type": "array"
					},
					"BaseUrl": {
						"type": "array"
					}
				},
				"folder": {
					"name": "Copy Demos/Federal Reserve"
				},
				"annotations": []
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/IterateAndCopySQLTables')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "IterateSQLTables",
						"type": "ForEach",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@pipeline().parameters.tableList",
								"type": "Expression"
							},
							"isSequential": true,
							"activities": [
								{
									"name": "Copy Data",
									"type": "Copy",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "SqlSource",
											"sqlReaderQuery": {
												"value": "SELECT * FROM [@{item().TABLE_SCHEMA}].[@{item().TABLE_NAME}]",
												"type": "Expression"
											},
											"partitionOption": "None"
										},
										"sink": {
											"type": "DelimitedTextSink",
											"storeSettings": {
												"type": "AzureBlobStorageWriteSettings"
											},
											"formatSettings": {
												"type": "DelimitedTextWriteSettings",
												"quoteAllText": true,
												"fileExtension": ".csv"
											}
										},
										"enableStaging": true,
										"stagingSettings": {
											"linkedServiceName": {
												"referenceName": "MyAzureBlobLinkedService",
												"type": "LinkedServiceReference"
											}
										},
										"dataIntegrationUnits": 0
									},
									"inputs": [
										{
											"referenceName": "AzureSqlDatabaseDataset",
											"type": "DatasetReference",
											"parameters": {}
										}
									],
									"outputs": [
										{
											"referenceName": "DelimitedText1",
											"type": "DatasetReference",
											"parameters": {}
										}
									]
								}
							]
						}
					}
				],
				"parameters": {
					"tableList": {
						"type": "Array"
					}
				},
				"folder": {
					"name": "Copy Demos"
				},
				"annotations": []
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/MergeCSVSampleData')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "Merge CSV Sample Data",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true,
									"wildcardFileName": "*"
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"sink": {
								"type": "DelimitedTextSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings",
									"copyBehavior": "MergeFiles"
								},
								"formatSettings": {
									"type": "DelimitedTextWriteSettings",
									"quoteAllText": true,
									"fileExtension": ".csv"
								}
							},
							"enableStaging": false
						},
						"inputs": [
							{
								"referenceName": "CSVSampleData",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "CSVSampleDataMergeSink",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					}
				],
				"annotations": []
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/PreCopyScriptTest')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "Copy SQL Table",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "AzureSqlSource",
								"queryTimeout": "02:00:00",
								"partitionOption": "None"
							},
							"sink": {
								"type": "AzureSqlSink",
								"tableOption": "autoCreate",
								"disableMetricsCollection": false
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "FactOrder",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "FactOrderSink",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "CreateCCI",
						"type": "Lookup",
						"dependsOn": [
							{
								"activity": "Copy SQL Table",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "AzureSqlSource",
								"sqlReaderQuery": "CREATE CLUSTERED COLUMNSTORE INDEX cci_FactOrder ON [Fact].[Order]",
								"queryTimeout": "02:00:00",
								"partitionOption": "None"
							},
							"dataset": {
								"referenceName": "FactOrderSink",
								"type": "DatasetReference",
								"parameters": {}
							},
							"firstRowOnly": false
						}
					}
				],
				"annotations": []
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/lnmyserver')]",
			"type": "Microsoft.DataFactory/factories/linkedServices",
			"apiVersion": "2018-06-01",
			"properties": {
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": "[parameters('lnmyserver_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AzureManagedVnet",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/CurrencyConverter')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "Sample demo data flow to convert currencies",
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "CurrencyDatasetUSD1",
								"type": "DatasetReference"
							},
							"name": "USDCurrency"
						},
						{
							"dataset": {
								"referenceName": "CurrencyDatasetCAD1",
								"type": "DatasetReference"
							},
							"name": "CADSource"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "USDOutput1",
								"type": "DatasetReference"
							},
							"name": "USDSink"
						},
						{
							"dataset": {
								"referenceName": "CADOutput1",
								"type": "DatasetReference"
							},
							"name": "CADSink"
						}
					],
					"transformations": [
						{
							"name": "Union",
							"description": "The Union combines 2 streams together"
						},
						{
							"name": "NewCurrencyColumn",
							"description": "Create a new calculated column from currency rate"
						},
						{
							"name": "ConditionalSplit1",
							"description": "Split the data on state to create 2 streams"
						}
					],
					"script": "source(output(\n\t\tColumn_1 as string,\n\t\tColumn_2 as string,\n\t\tColumn_3 as string,\n\t\tColumn_4 as string\n\t),\n\tallowSchemaDrift: false,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: false) ~> USDCurrency\nsource(output(\n\t\tPreviousConversionRate as double,\n\t\tCountry as string,\n\t\tDateTime1 as string,\n\t\tCurrentConversionRate as double\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: false) ~> CADSource\nUSDCurrency, CADSource union(byName: true)~> Union\nUnion derive(NewCurrencyRate = round(CurrentConversionRate*1.25)) ~> NewCurrencyColumn\nNewCurrencyColumn split(Country == 'USD',\n\tCountry == 'CAD',\n\tdisjoint: false) ~> ConditionalSplit1@(USD, CAD)\nConditionalSplit1@USD sink(allowSchemaDrift: true,\n\tvalidateSchema: false) ~> USDSink\nConditionalSplit1@CAD sink(allowSchemaDrift: true,\n\tvalidateSchema: false) ~> CADSink"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/DB2DateFixer')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "AzureBlobDB2Source",
								"type": "DatasetReference"
							},
							"name": "AzureBlobDB2Source"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "AzureBlobDB2Sink",
								"type": "DatasetReference"
							},
							"name": "AzureBlobDB2DateFixed"
						}
					],
					"transformations": [
						{
							"name": "Db2ToSQLDateFormat"
						},
						{
							"name": "Db2ToSQLTimestamp"
						},
						{
							"name": "DB224TimeStamp"
						},
						{
							"name": "DB224Time"
						}
					],
					"script": "parameters{\n\tOutputFileName as string\n}\nsource(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: false,\n\tinferDriftedColumnTypes: true) ~> AzureBlobDB2Source\nDB224Time derive(each(match(type=='string'), $$ = regexReplace($$, `(\\d+)\\.(\\d+)\\.(\\d+)`, '$1:$2:$3'))) ~> Db2ToSQLDateFormat\nDB224TimeStamp derive(each(match(type=='string'), $$ = regexReplace($$, `(\\-)(\\d+)\\.(\\d+)\\.(\\d+)`, ' $2:$3:$4'))) ~> Db2ToSQLTimestamp\nAzureBlobDB2Source derive(each(match(type=='string'), $$ = regexReplace($$, `(\\\\d+\\\\-\\\\d+\\\\-\\\\d+)\\\\-24\\\\.00\\\\.00\\\\.000000`, '$1 23:59:59:999999'))) ~> DB224TimeStamp\nDb2ToSQLTimestamp derive(each(match(type=='string'), $$ = regexReplace($$, `(24\\\\.00\\\\.00`, '23:59:59.999999'))) ~> DB224Time\nDb2ToSQLDateFormat sink(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tpartitionFileNames:[($OutputFileName)],\n\tpartitionBy('hash', 1),\n\tskipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: true,\n\tquoteAll: true) ~> AzureBlobDB2DateFixed"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/DateFixer')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "AzureBlobDB2Source",
								"type": "DatasetReference"
							},
							"name": "AzureBlobDB2Export"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "AzureBlobDB2Sink",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "DerivedColumn1"
						}
					],
					"script": "source(output(\n\t\t{_col0_} as string,\n\t\t{_col1_} as integer,\n\t\t{_col2_} as string,\n\t\t{_col3_} as string,\n\t\t{_col4_} as date,\n\t\t{_col5_} as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: false,\n\tinferDriftedColumnTypes: true) ~> AzureBlobDB2Export\nAzureBlobDB2Export derive({_col5_} = regexReplace({_col5_}, '(\\\\d+).(\\\\d+).(\\\\d+)', '$1:$2:$3')) ~> DerivedColumn1\nDerivedColumn1 sink(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tpartitionFileNames:['APPOINTMENT_ZONE.DAT'],\n\tpartitionBy('hash', 1),\n\tskipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: true) ~> sink1"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/FixedWidth')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "Each parameter in this data flow is defined as 'start position', 'offset' as in '1,7'.",
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "fixedwidth1",
								"type": "DatasetReference"
							},
							"name": "fixedsource1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "folderout1",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "MapFields"
						}
					],
					"script": "parameters{\n\tField1 as string ('1,7'),\n\tField2 as string ('8,8'),\n\tField3 as string ('15,10'),\n\tField4 as string ('25,11'),\n\tField5 as string ('36,10'),\n\tField6 as string ('46,12'),\n\tField7 as string ('58,1')\n}\nsource(output(\n\t\tColumn_1 as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: false) ~> fixedsource1\nfixedsource1 derive(Field1 = substring(Column_1,toInteger(split($Field1,',')[1]),toInteger(split($Field1,',')[2])),\n\t\tField2 = substring(Column_1,toInteger(split($Field2,',')[1]),toInteger(split($Field2,',')[2])),\n\t\tField3 = substring(Column_1,toInteger(split($Field3,',')[1]),toInteger(split($Field3,',')[2])),\n\t\tField4 = substring(Column_1,toInteger(split($Field4,',')[1]),toInteger(split($Field4,',')[2])),\n\t\tField5 = substring(Column_1,toInteger(split($Field5,',')[1]),toInteger(split($Field5,',')[2])),\n\t\tField6 = substring(Column_1,toInteger(split($Field6,',')[1]),toInteger(split($Field6,',')[2])),\n\t\tField7 = substring(Column_1,toInteger(split($Field7,',')[1]),toInteger(split($Field7,',')[2])),\n\tpartitionBy('roundRobin', 2)) ~> MapFields\nMapFields sink(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tpartitionFileNames:['mysinglefile.csv'],\n\ttruncate: true,\n\tpartitionBy('hash', 1)) ~> sink1"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/MovieDemo')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "movie_dataflow_source1",
								"type": "DatasetReference"
							},
							"name": "Movies"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "movie_dataflow_sink1",
								"type": "DatasetReference"
							},
							"name": "Output"
						}
					],
					"transformations": [
						{
							"name": "MoviesYear"
						}
					],
					"script": "source(output(\n\t\tmovieId as string,\n\t\ttitle as string,\n\t\tgenres as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: false) ~> Movies\nMovies derive(year = toInteger(trim(right(title, 6), '()')),\n\t\ttitle = toString(left(title, length(title)-6))) ~> MoviesYear\nMoviesYear sink(allowSchemaDrift: true,\n\tvalidateSchema: false) ~> Output"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/STLFED')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "StagingParquet1",
								"type": "DatasetReference"
							},
							"name": "STLPCPI"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "AzureSqlDatabaseDataset_New1",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "ConverToInts"
						}
					],
					"script": "source(output(\n\t\tDATE as string,\n\t\tSTLPCPI_20171116 as string,\n\t\tSTLPCPI_20181115 as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: false,\n\tformat: 'parquet') ~> STLPCPI\nSTLPCPI derive(DATE = toDate(DATE),\n\t\tSTLPCPI_20171116 = toInteger(STLPCPI_20171116),\n\t\tSTLPCPI_20181115 = toInteger(STLPCPI_20181115)) ~> ConverToInts\nConverToInts sink(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tdeletable:false,\n\tinsertable:true,\n\tupdateable:false,\n\tupsertable:false,\n\tformat: 'table',\n\terrorHandlingOption: 'stopOnFirstError') ~> sink1"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/SearchLog')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "searchLog1",
								"type": "DatasetReference"
							},
							"name": "searchLog"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "AzureSqlDWTable11_New1",
								"type": "DatasetReference"
							},
							"name": "sinkIntoDW"
						}
					],
					"transformations": [
						{
							"name": "totalDurationByRegion"
						},
						{
							"name": "RenameColumns"
						},
						{
							"name": "DateFilter"
						},
						{
							"name": "ConvertDate"
						},
						{
							"name": "DurationFilter"
						}
					],
					"script": "source(output(\n\t\t{_col0_} as integer,\n\t\t{_col1_} as string,\n\t\t{_col2_} as string,\n\t\t{_col3_} as string,\n\t\t{_col4_} as integer,\n\t\t{_col5_} as string,\n\t\t{_col6_} as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: false) ~> searchLog\nDateFilter aggregate(groupBy(region),\n\ttotalduration = sum(duration)) ~> totalDurationByRegion\nsearchLog select(mapColumn(\n\t\tuserid = {_col0_},\n\t\tstart = {_col1_},\n\t\tregion = {_col2_},\n\t\tquery = {_col3_},\n\t\tduration = {_col4_},\n\t\turls = {_col5_},\n\t\tclickedurls = {_col6_}\n\t),\n\tskipDuplicateMapInputs: false,\n\tskipDuplicateMapOutputs: false) ~> RenameColumns\nConvertDate filter(newdate > toDate('2012-02-06','yyyy-MM-dd')) ~> DateFilter\nRenameColumns derive(newdate = toDate(left(start,instr(start,' ')-1),'MM/dd/yyyy')) ~> ConvertDate\ntotalDurationByRegion filter(totalduration > 200) ~> DurationFilter\nDurationFilter sink(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tdeletable:false,\n\tinsertable:true,\n\tupdateable:false,\n\tupsertable:false,\n\tformat: 'table',\n\tstaged: false) ~> sinkIntoDW"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/TaxiDemo')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "taxi_trip_data_input1",
								"type": "DatasetReference"
							},
							"name": "TripData"
						},
						{
							"dataset": {
								"referenceName": "taxi_trip_fare_input1",
								"type": "DatasetReference"
							},
							"name": "TripFare"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "TaxiDemoVendorStatsSink1",
								"type": "DatasetReference"
							},
							"name": "VendorStatsSink"
						},
						{
							"dataset": {
								"referenceName": "TaxiDemoDayStatsSink1",
								"type": "DatasetReference"
							},
							"name": "DayStatsSink"
						},
						{
							"dataset": {
								"referenceName": "TaxiDemoTotalByPaymentType1",
								"type": "DatasetReference"
							},
							"name": "TotalPaymentByPaymentType"
						}
					],
					"transformations": [
						{
							"name": "JoinMatchedData"
						},
						{
							"name": "AggregateVendorStats"
						},
						{
							"name": "AggregateDayStats"
						},
						{
							"name": "AggregateByPaymentType"
						}
					],
					"script": "source(output(\n\t\tmedallion as string,\n\t\thack_license as string,\n\t\tvendor_id as string,\n\t\trate_code as string,\n\t\tstore_and_fwd_flag as string,\n\t\tpickup_datetime as string,\n\t\tdropoff_datetime as string,\n\t\tpassenger_count as short,\n\t\ttrip_time_in_secs as long,\n\t\ttrip_distance as double,\n\t\tpickup_longitude as double,\n\t\tpickup_latitude as double,\n\t\tdropoff_longitude as double,\n\t\tdropoff_latitude as double\n\t),\n\tallowSchemaDrift: false,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: false) ~> TripData\nsource(output(\n\t\tmedallion as string,\n\t\t{ hack_license} as string,\n\t\t{ vendor_id} as string,\n\t\t{ pickup_datetime} as string,\n\t\t{ payment_type} as string,\n\t\t{ fare_amount} as double,\n\t\t{ surcharge} as double,\n\t\t{ mta_tax} as double,\n\t\t{ tip_amount} as double,\n\t\t{ tolls_amount} as double,\n\t\t{ total_amount} as double\n\t),\n\tallowSchemaDrift: false,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: false) ~> TripFare\nTripData, TripFare join(hack_license == { hack_license}\n\t&& TripData@medallion == TripFare@medallion\n\t&& vendor_id == { vendor_id}\n\t&& pickup_datetime == { pickup_datetime},\n\tjoinType:'inner',\n\tbroadcast: 'auto')~> JoinMatchedData\nJoinMatchedData aggregate(groupBy(vendor_id),\n\tpassenger_count = round(sum(passenger_count), 2),\n\t\ttrip_time_in_secs = round(sum(trip_time_in_secs)/60, 2),\n\t\ttrip_distance = round(sum(trip_distance), 2),\n\t\tTotalTripFare = round(sum({ total_amount}), 2)) ~> AggregateVendorStats\nJoinMatchedData aggregate(groupBy(DayOfTheWeek = dayOfWeek(toDate(pickup_datetime,'yyyy-mm-dd hh:mm:ss'))),\n\ttrip_distance = round(avg(trip_distance), 2),\n\t\tpassenger_count = round(avg(passenger_count), 2),\n\t\ttrip_time_in_secs = round(avg(trip_time_in_secs)/60, 2),\n\t\taverage_fare = round(avg({ total_amount}), 2)) ~> AggregateDayStats\nTripFare aggregate(groupBy({ payment_type}),\n\teach(match(type=='double'), concat($$, '_total') = round(sum ($$)))) ~> AggregateByPaymentType\nAggregateVendorStats sink(allowSchemaDrift: true,\n\tvalidateSchema: false) ~> VendorStatsSink\nAggregateDayStats sink(allowSchemaDrift: true,\n\tvalidateSchema: false) ~> DayStatsSink\nAggregateByPaymentType sink(allowSchemaDrift: true,\n\tvalidateSchema: false) ~> TotalPaymentByPaymentType"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/dedupeProb')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "names1001",
								"type": "DatasetReference"
							},
							"name": "sourceName"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "dupefolder1",
								"type": "DatasetReference"
							},
							"name": "sinkDupes"
						},
						{
							"dataset": {
								"referenceName": "dupefolder1",
								"type": "DatasetReference"
							},
							"name": "sinkNoDupes"
						}
					],
					"transformations": [
						{
							"name": "FuzzyMatch"
						},
						{
							"name": "groupSoundex"
						},
						{
							"name": "Orig1"
						},
						{
							"name": "soundexJoin"
						},
						{
							"name": "soundexBranch"
						},
						{
							"name": "groupPhone"
						},
						{
							"name": "phoneBranch"
						},
						{
							"name": "phoneJoin"
						},
						{
							"name": "groupZip"
						},
						{
							"name": "zipBranch"
						},
						{
							"name": "zipJoin"
						},
						{
							"name": "setConstants"
						},
						{
							"name": "matchScore"
						},
						{
							"name": "finalResult"
						},
						{
							"name": "MapNames"
						},
						{
							"name": "CreateFullName"
						},
						{
							"name": "CheckForDupes"
						}
					],
					"script": "source(output(\n\t\t{Emp ID} as integer,\n\t\t{Name Prefix} as string,\n\t\t{First Name} as string,\n\t\t{Middle Initial} as string,\n\t\t{Last Name} as string,\n\t\tGender as string,\n\t\t{E Mail} as string,\n\t\t{Father's Name} as string,\n\t\t{Mother's Name} as string,\n\t\t{Mother's Maiden Name} as string,\n\t\t{Date of Birth} as date,\n\t\t{Time of Birth} as string,\n\t\t{Age in Yrs.} as double,\n\t\t{Weight in Kgs.} as integer,\n\t\t{Date of Joining} as date,\n\t\t{Quarter of Joining} as string,\n\t\t{Half of Joining} as string,\n\t\t{Year of Joining} as short,\n\t\t{Month of Joining} as short,\n\t\t{Month Name of Joining} as string,\n\t\t{Short Month} as string,\n\t\t{Day of Joining} as short,\n\t\t{DOW of Joining} as string,\n\t\t{Short DOW} as string,\n\t\t{Age in Company (Years)} as double,\n\t\tSalary as integer,\n\t\t{Last % Hike} as string,\n\t\tSSN as string,\n\t\t{Phone No. } as string,\n\t\t{Place Name} as string,\n\t\tCounty as string,\n\t\tCity as string,\n\t\tState as string,\n\t\tZip as integer,\n\t\tRegion as string,\n\t\t{User Name} as string,\n\t\tPassword as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: false) ~> sourceName\nMapNames derive(SoundexValue = soundex(fullname)) ~> FuzzyMatch\nOrig1 aggregate(groupBy(SoundexValue),\n\tsoundexmatch = sum(1)) ~> groupSoundex\nFuzzyMatch select(mapColumn(\n\t\tacctnum,\n\t\tfullname,\n\t\tphone,\n\t\tzip,\n\t\tSoundexValue\n\t),\n\tskipDuplicateMapInputs: false,\n\tskipDuplicateMapOutputs: false) ~> Orig1\ngroupSoundex, soundexBranch join(groupSoundex@SoundexValue == soundexBranch@SoundexValue,\n\tjoinType:'inner',\n\tbroadcast: 'auto')~> soundexJoin\nFuzzyMatch select(mapColumn(\n\t\tacctnum,\n\t\tfullname,\n\t\tphone,\n\t\tzip,\n\t\tSoundexValue\n\t),\n\tskipDuplicateMapInputs: false,\n\tskipDuplicateMapOutputs: false) ~> soundexBranch\nsoundexJoin aggregate(groupBy(phone,\n\t\tsoundexBranch@SoundexValue),\n\tphonematch = sum(1),\n\t\tacctnum_agg = last(acctnum)) ~> groupPhone\nsoundexJoin select(mapColumn(\n\t\tsoundexmatch,\n\t\tacctnum,\n\t\tfullname,\n\t\tphone,\n\t\tzip,\n\t\tSoundexValue = soundexBranch@SoundexValue\n\t),\n\tskipDuplicateMapInputs: false,\n\tskipDuplicateMapOutputs: false) ~> phoneBranch\ngroupPhone, phoneBranch join(acctnum_agg == acctnum,\n\tjoinType:'right',\n\tbroadcast: 'auto')~> phoneJoin\nphoneJoin aggregate(groupBy(zip,\n\t\tphoneBranch@SoundexValue),\n\tzipcount = sum(1),\n\t\tacctnum_agg = last(acctnum_agg)) ~> groupZip\nphoneJoin select(mapColumn(\n\t\tphonematch,\n\t\tsoundexmatch,\n\t\tacctnum,\n\t\tfullname,\n\t\tphone = phoneBranch@phone,\n\t\tzip,\n\t\tSoundexValue = phoneBranch@SoundexValue\n\t),\n\tskipDuplicateMapInputs: false,\n\tskipDuplicateMapOutputs: false) ~> zipBranch\ngroupZip, zipBranch join(acctnum_agg == acctnum,\n\tjoinType:'right',\n\tbroadcast: 'auto')~> zipJoin\nzipJoin derive(soundexweight = 50,\n\t\tzipweight = 25,\n\t\tphoneweight = 25,\n\t\tsoundexbool = iif (soundexmatch > 1, 1, 0),\n\t\tzipbool = iif (zipcount > 1, 1, 0),\n\t\tphonebool = iif (phonematch > 1, 1, 0)) ~> setConstants\nsetConstants derive(matchscore = (soundexbool * 50) + (zipbool * 25) + (phonebool * 25)) ~> matchScore\nmatchScore select(mapColumn(\n\t\tphone,\n\t\tacctnum,\n\t\tfullname,\n\t\tzip = zipBranch@zip,\n\t\tmatchscore\n\t),\n\tskipDuplicateMapInputs: false,\n\tskipDuplicateMapOutputs: false) ~> finalResult\nCreateFullName select(mapColumn(\n\t\tphone = {Phone No. },\n\t\tzip = Zip,\n\t\tfullname,\n\t\tacctnum = {Emp ID}\n\t),\n\tskipDuplicateMapInputs: false,\n\tskipDuplicateMapOutputs: false) ~> MapNames\nsourceName derive(fullname = {First Name} + ' ' + {Last Name}) ~> CreateFullName\nfinalResult split(matchscore > 50,\n\tdisjoint: false) ~> CheckForDupes@(Duplicates, NotDupe)\nCheckForDupes@Duplicates sink(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tpartitionFileNames:['dupes.csv'],\n\tpartitionBy('hash', 1)) ~> sinkDupes\nCheckForDupes@NotDupe sink(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tpartitionFileNames:['nodupes.csv'],\n\tpartitionBy('hash', 1)) ~> sinkNoDupes"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/CurrencyConverter')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "Currency Converter1",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "CurrencyConverter",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"USDCurrency": {},
									"CADSource": {},
									"USDSink": {},
									"CADSink": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"folder": {
					"name": "Data Flow Demos"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/dataflows/CurrencyConverter')]"
			]
		}
	]
}